{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iC5jzImnEKY"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# List of required packages\n",
        "required_packages = ['requests', 'beautifulsoup4', 'pandas']\n",
        "\n",
        "# Function to install packages if they are not already installed\n",
        "def install_package(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Install any missing packages\n",
        "for package in required_packages:\n",
        "    install_package(package)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wVKReK8lnEKf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_bKqsN3nEKg",
        "outputId": "05c3e1a2-978e-4139-8007-31e4da0aec3e"
      },
      "outputs": [],
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 5.1.1; SM-G928X Build/LMY47X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.83 Mobile Safari/537.36'}\n",
        "start_url = \"https://guide.michelin.com/en/it/restauranttttts\"\n",
        "base_url = \"https://guide.michelin.com\"\n",
        "next_page = start_url\n",
        "link_list = []\n",
        "while next_page:\n",
        "    # Request page content\n",
        "    response = requests.get(next_page,verify=False, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, features=\"lxml\")\n",
        "    # Find all restaurant links on the current page\n",
        "    for link in soup.select(\"a.link\"):\n",
        "        href = link.get(\"href\")\n",
        "        if href and \"/restaurant/\" in href:\n",
        "            link_list.append(base_url + href)\n",
        "    # Look for the 'Next' button to proceed to the next page\n",
        "    next_button = soup.find_all(\"a\", class_=\"btn btn-outline-secondary btn-sm btn-carousel__link\", href=True)\n",
        "    if next_button:\n",
        "        for content in next_button:\n",
        "            if content.find(\"span\", class_=\"icon fal fa-angle-right\"):\n",
        "                next_page = base_url+content[\"href\"]\n",
        "                break\n",
        "            else:\n",
        "                next_page = None\n",
        "\n",
        "    else:\n",
        "        next_page = None\n",
        "\n",
        "\n",
        "# Display the collected links\n",
        "print(f\"Found {len(link_list)} restaurants:\")\n",
        "# Save to a text file\n",
        "with open(\"urls.txt\", \"w\") as file:\n",
        "    for url in link_list:\n",
        "        file.write(f\"{url}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4E4P3XsgnEKi",
        "outputId": "421deac5-c175-4358-abc8-1dd68fbcfc79"
      },
      "outputs": [],
      "source": [
        "# it is gonna take more 10 minutes\n",
        "for index, link in enumerate(link_list):\n",
        "    cnt = requests.get(link, verify=False, headers=headers)\n",
        "    if cnt.status_code==200:\n",
        "        html = BeautifulSoup(cnt.content, features=\"lxml\")\n",
        "        # Define the name of the subfolder and the filename\n",
        "        subfolder = f\"HTML/Page {str((index+20)//20)}\"\n",
        "        filename = f\"{(link[link.rfind('/') + 1:]).replace('-', ' ')}.html\"\n",
        "        file_path = os.path.join(subfolder, filename)\n",
        "\n",
        "        # Check if the subfolder exists, create it if it doesn't\n",
        "        if not os.path.exists(subfolder):\n",
        "            os.makedirs(subfolder)\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(html.prettify())\n",
        "    else:\n",
        "        print(\"Request denied!\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "AzM4JctNnEKj",
        "outputId": "4a2844ef-b2c0-48ae-a6f2-155ac53e1031"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize an empty list to store the rows for DataFrame\n",
        "data = []\n",
        "# Define the base directory\n",
        "base_directory = \"HTML\"\n",
        "# Use glob to find all directories matching \"Page*\"\n",
        "page_folders = glob.glob(os.path.join(base_directory, \"Page *\"))\n",
        "\n",
        "# Loop through each Page* directory\n",
        "for page_folder in page_folders:\n",
        "    # Get all HTML files in the current Page* directory\n",
        "    html_files = glob.glob(os.path.join(page_folder, \"*.html\"))\n",
        "\n",
        "    # Read each HTML file\n",
        "    for html_file in html_files:\n",
        "        with open(html_file, \"r\", encoding='utf-8') as file:  # Ensure correct encoding\n",
        "            content = BeautifulSoup(file.read(), \"html.parser\")\n",
        "            # Extract the required information\n",
        "            restaurantName = content.find(\"h1\",class_=\"data-sheet__title\").get_text().strip() if content.find(\"h1\",class_=\"data-sheet__title\") else \"\"\n",
        "\n",
        "            basic_info_first_row_list=content.findAll(\"div\",class_=\"data-sheet__block--text\")[0].text\n",
        "            basic_info_first_row_striped_list = [info.strip() for info in basic_info_first_row_list.split(\",\")]\n",
        "            address = \" \".join(basic_info_first_row_striped_list[:-3]) if basic_info_first_row_striped_list[:-3] else \"\"\n",
        "            city = basic_info_first_row_striped_list[-3] if basic_info_first_row_striped_list[-3] else \"\"\n",
        "            postal_code = basic_info_first_row_striped_list[-2]  if basic_info_first_row_striped_list[-2] else \"\"\n",
        "            country = basic_info_first_row_striped_list[-1]  if basic_info_first_row_striped_list[-1] else \"\"\n",
        "\n",
        "\n",
        "            basic_info_second_row_list=content.findAll(\"div\",class_=\"data-sheet__block--text\")[1].text\n",
        "            basic_info_second_row_striped_list = [info.strip() for info in basic_info_second_row_list.split(\"Â·\")]\n",
        "\n",
        "            priceRange = basic_info_second_row_striped_list[0] if basic_info_second_row_striped_list[0] else \"\"\n",
        "            cuisineType = basic_info_second_row_striped_list[1]  if basic_info_second_row_striped_list[1] else \"\"\n",
        "\n",
        "            description = content.find(\"div\",class_=\"data-sheet__description\").get_text().strip() if content.find(\"div\",class_=\"data-sheet__description\") else \"\"\n",
        "\n",
        "            facilitiesServices_div = content.findAll(\"div\", class_=\"col col-12 col-lg-6\")\n",
        "            facilitiesServices = [li.get_text(strip=True) for li in facilitiesServices_div[0].find_all(\"li\")] if facilitiesServices_div[0] else \"\"\n",
        "\n",
        "            div_creditCard = content.find(\"div\", class_=\"restaurant-details__services--info\")\n",
        "\n",
        "            creditCards = [os.path.basename(img['data-src']).split('-')[0] for img in div_creditCard.find_all(\"img\")] if div_creditCard else \"\"\n",
        "\n",
        "\n",
        "            phoneNumber = content.find(\"span\", attrs={\"x-ms-format-detection\": \"none\"}).get_text().strip() if content.find(\"span\", attrs={\"x-ms-format-detection\": \"none\"}) else \"\"\n",
        "\n",
        "\n",
        "            div_website = content.find(\"div\", class_=\"collapse__block-item link-item\")\n",
        "\n",
        "            # Find the <a> tag within this container and get the href attribute\n",
        "            a_website = div_website.find(\"a\", class_=\"link js-dtm-link\") if div_website else \"\"\n",
        "            website = a_website.get(\"href\") if a_website!=\"\" else \"\"\n",
        "\n",
        "\n",
        "            # Append the extracted info as a new row to the list\n",
        "            data.append([restaurantName,address,city,postal_code,country,priceRange,cuisineType,description,facilitiesServices,creditCards,phoneNumber,website])\n",
        "\n",
        "\n",
        "# Create a DataFrame from the data list\n",
        "df = pd.DataFrame(data, columns=[\"restaurantName\",\"Address\",\"City\",\"Postal Code\",\"Country\",\"Price Range\",\"Cuisine Type\",\"Description\",\"facilitiesServices\",\"creditCards\",\"phoneNumber\",\"website\"])\n",
        "\n",
        "display(df)\n",
        "\n",
        "\n",
        "\n",
        "# Iterate through each row in the DataFrame\n",
        "for i, row in df.iterrows():\n",
        "    # Define the file name using the index\n",
        "    file_name = f\"restaurant_{i}.tsv\"\n",
        "\n",
        "    # Prepare row data as a single line with tab-separated values\n",
        "    content =  f\"{row['restaurantName']}\\t{row['Address']}\\t{row['City']}\\t{row['Postal Code']}\\t{row['Country']}\\t{row['Price Range']}\\t{row['Cuisine Type']}\\t{row['Description']}\\t{row['facilitiesServices']}\\t{row['creditCards']}\\t{row['phoneNumber']}\\t{row['website']}\\n\"\n",
        "\n",
        "    subfolder = f\"tsv_files\"\n",
        "    file_path = os.path.join(subfolder, file_name)\n",
        "\n",
        "    # Check if the subfolder exists, create it if it doesn't\n",
        "    if not os.path.exists(subfolder):\n",
        "        os.makedirs(subfolder)\n",
        "    # Write the row data to the .tsv file\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(content)\n",
        "\n",
        "    print(f\"Created file: {file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIyZZuKhs_M1"
      },
      "source": [
        "## ***2.0 Pre-processing:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KsJXP6krdOv",
        "outputId": "211a1362-5a2c-4135-f679-04aed0d6b6ff"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q5f4D_L820QR"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import words\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "english_words = set(words.words())  \n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    words = word_tokenize(text)\n",
        "    words = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in words\n",
        "        if word not in stop_words and len(word) > 2 and word in english_words\n",
        "    ]\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "collapsed": true,
        "id": "gyMJ-dpmsbU8",
        "outputId": "1125937a-4f97-4bd5-b4c5-fee536a43eaf"
      },
      "outputs": [],
      "source": [
        "df['Description'] = df['Description'].str.lower()\n",
        "df['processed_description'] = df['Description'].apply(preprocess_text)\n",
        "df['processed_description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "PT6z9UFoebY_",
        "outputId": "bf71af62-6b53-4fbb-edd4-f9fb119a3b75"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCIGeLgctMeQ"
      },
      "source": [
        "## ***2.1 Conjuctive Query:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MZZXm2OJebZA",
        "outputId": "15912198-5b15-4122-ccdf-822ba49c1afe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "vocab_dict = {}\n",
        "term_id = 0\n",
        "\n",
        "all_uniqe_words = set(word for description in df[\"processed_description\"] for word in description)\n",
        "\n",
        "for word in all_uniqe_words:\n",
        "    vocab_dict[word] = term_id\n",
        "    term_id+=1\n",
        "\n",
        "vocab_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "98YfOyzEebZA",
        "outputId": "490ffa8f-8e42-40ae-aa30-f6bba5bda022"
      },
      "outputs": [],
      "source": [
        "vocab_df = pd.DataFrame(list(vocab_dict.items()), columns=['term', 'term_id'])\n",
        "vocab_df.to_csv('vocabulary.csv', index=False)\n",
        "\n",
        "vocab_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dgk-ATaftjCF"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "inverted_idx = defaultdict(list)\n",
        "\n",
        "for idx, description in enumerate(df['processed_description']):\n",
        "    for word in description:\n",
        "        term_id = vocab_dict[word]\n",
        "\n",
        "        if idx not in inverted_idx[term_id]:\n",
        "            inverted_idx[term_id].append(idx)\n",
        "\n",
        "\n",
        "with open('inverted_index.json', 'w') as f:\n",
        "    json.dump(inverted_idx, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L7uDREv9ebZC"
      },
      "outputs": [],
      "source": [
        "def process_query(query, vocab_dict, inverted_index, df):\n",
        "    query_words = preprocess_text(query)\n",
        "\n",
        "    doc_sets = []\n",
        "    for word in query_words:\n",
        "        term_id = vocab_dict.get(word)\n",
        "        if term_id is not None:\n",
        "            doc_sets.append(set(inverted_index.get(term_id, [])))\n",
        "\n",
        "    if doc_sets:\n",
        "        result_docs = set.intersection(*doc_sets)\n",
        "    else:\n",
        "        result_docs = set()\n",
        "\n",
        "    results = df.loc[result_docs, [\"restaurantName\", \"Address\", \"Description\", \"website\"]]\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "ZZtoLGIJebZC",
        "outputId": "0d1a830c-294b-44f9-ec93-1220a9ac0860"
      },
      "outputs": [],
      "source": [
        "query = \"modern seasonal cuisine\"\n",
        "results = process_query(query, vocab_dict, inverted_idx, df)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgCbr54uebZD"
      },
      "source": [
        "## ***2.2 Ranked Search Engine with TF-IDF and Cosine Similarity:***\n",
        "\n",
        "BY ME\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hBBt5UouebZD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def build_tfidf_inverted_index(df):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(df['processed_description'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    term_to_id = {term: idx for idx, term in enumerate(terms)}\n",
        "    inverted_index = defaultdict(list)\n",
        "\n",
        "    for doc_id in range(tfidf_matrix.shape[0]):\n",
        "        for term_id in tfidf_matrix[doc_id].nonzero()[1]:\n",
        "            tfidf_score = tfidf_matrix[doc_id, term_id]\n",
        "            inverted_index[term_id].append((doc_id, tfidf_score))\n",
        "\n",
        "    return inverted_index, term_to_id, tfidf_matrix, vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zMm_KnrYebZE",
        "outputId": "d789d9f5-6543-41f9-bcbe-15ffd02937cf"
      },
      "outputs": [],
      "source": [
        "inverted_index, term_to_id, tfidf_matrix, vectorizer = build_tfidf_inverted_index(df)\n",
        "\n",
        "readable_inverted_index = {\n",
        "    term: [(doc_id, round(tfidf_score, 3)) for doc_id, tfidf_score in inverted_index[term_id]]\n",
        "    for term, term_id in term_to_id.items()\n",
        "}\n",
        "\n",
        "for term, doc_scores in readable_inverted_index.items():\n",
        "    print(f\"{term}: {doc_scores}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rCaXxbSuebZF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def search_query(query, tfidf_matrix, vectorizer, df, top_k=5):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "\n",
        "    cosine_similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
        "\n",
        "    relevant_docs = np.argsort(-cosine_similarities)[:top_k]\n",
        "\n",
        "    results = df.loc[relevant_docs, [\"restaurantName\", \"Address\", \"Description\", \"website\"]]\n",
        "    results[\"Similarity Score\"] = cosine_similarities[relevant_docs]\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4O6w-00ebZF"
      },
      "source": [
        "***TEST***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "collapsed": true,
        "id": "q_h9bu3tebZF",
        "outputId": "69fd4dbf-4450-45e5-9d2f-b865fff441d9"
      },
      "outputs": [],
      "source": [
        "inverted_index, term_to_id, tfidf_matrix, vectorizer = build_tfidf_inverted_index(df)\n",
        "\n",
        "query = \"modern seasonal cuisine\"\n",
        "results = search_query(query, tfidf_matrix, vectorizer, df)\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sY6zzeG8HrKv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def search_query_dscore(query, tfidf_matrix, vectorizer, df, top_k=5):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "\n",
        "    cosine_similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
        "\n",
        "    return cosine_similarities\n",
        "\n",
        "def calculate_score(doc, query, vectorizer, tfidf_matrix, cuisine_preferences, facility_preferences, price_preferences):\n",
        "    score = 0\n",
        "\n",
        "    # TF-IDF vector for the query\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "\n",
        "\n",
        "    doc_index = doc.name  # Index of the document in the dataframe\n",
        "    doc_vec = tfidf_matrix[doc_index]\n",
        "\n",
        "    #description score\n",
        "    description_score = cosine_similarity(query_tfidf, doc_vec)[0, 0]  # cosine similarity\n",
        "    score += description_score\n",
        "\n",
        "\n",
        "    #cuisine type score\n",
        "    for cuisine in cuisine_preferences:\n",
        "        if cuisine.lower() in doc['Cuisine Type'].lower():\n",
        "            score += 0.2\n",
        "\n",
        "    #facilities score\n",
        "    for facility in facility_preferences:\n",
        "        if facility.lower() in [f.lower() for f in doc['facilitiesServices']]:\n",
        "            score += 0.1\n",
        "\n",
        "    #price range score\n",
        "    for price in price_preferences:\n",
        "        if price in doc['Price Range']:\n",
        "            score += 0.2\n",
        "\n",
        "    return score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wNp6ex6IKayy"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "def ranked_restaurants(query, tfidf_matrix, vectorizer, df, top_k=5, cuisine_preferences=None, facility_preferences=None, price_preferences=None):\n",
        "    # create an heap\n",
        "    heap = []\n",
        "\n",
        "    for doc_id, doc in df.iterrows():  # for each restaurant\n",
        "        # personalized score\n",
        "        score = calculate_score(\n",
        "            doc,  # current restourant\n",
        "            query,\n",
        "            vectorizer,\n",
        "            tfidf_matrix,\n",
        "            cuisine_preferences or [],\n",
        "            facility_preferences or [],\n",
        "            price_preferences or []\n",
        "        )\n",
        "\n",
        "        # top k results in the heat\n",
        "        if len(heap) < top_k:\n",
        "            heapq.heappush(heap, (score, doc_id))  # Adding an element (score, ID doc)\n",
        "        else:\n",
        "            heapq.heappushpop(heap, (score, doc_id))  # Replace the smaller item if necessary\n",
        "\n",
        "    # Sort the heap to get the results in descending order\n",
        "    ranked_results = sorted(heap, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Format the final results\n",
        "    results = []\n",
        "    for score, doc_id in ranked_results:\n",
        "        row = df.iloc[doc_id]\n",
        "        results.append({\n",
        "            \"restaurantName\": row[\"restaurantName\"],\n",
        "            \"Address\": row[\"Address\"],\n",
        "            \"Description\": row[\"Description\"],\n",
        "            \"website\": row[\"website\"],\n",
        "            \"custom_score\": round(score, 3)\n",
        "        })\n",
        "\n",
        "    results_df2 = pd.DataFrame(results)\n",
        "    return results_df2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "w4GTIjq4l6IU",
        "outputId": "91379f9c-af36-4f9f-8192-6af50e447c5e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "query = \"modern seasonal cuisine\"\n",
        "cuisine_preferences = [\"Italian\", \"French\"]\n",
        "facility_preferences = [\"Terrace\", \"Air conditioning\"]\n",
        "price_preferences = [\"$$\", \"$$$\"]\n",
        "top_k = 5\n",
        "\n",
        "results_df = ranked_restaurants(query, tfidf_matrix, vectorizer, df, top_k, cuisine_preferences, facility_preferences, price_preferences)\n",
        "\n",
        "# Use display for a tabular view\n",
        "display(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4JNGJOyPpfu"
      },
      "source": [
        "**The new scoring function improves the results because it incorporates additional variables such as the type of cuisine, the services available and the price range. This allows us to obtain results that are more relevant to the user's preferences, which would otherwise have been ignored in the original scoring function. For example, a restaurant that meets preferences in terms of cuisine and price, but has a less detailed description, is now considered more relevant.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_region(adres):\n",
        "    url = \"https://nominatim.openstreetmap.org/search\"\n",
        "    params = {\n",
        "        'q': adres,\n",
        "        'format': 'json',\n",
        "        'addressdetails': 1,\n",
        "        'limit': 1\n",
        "    }\n",
        "    headers = {\n",
        "        'User-Agent': 'sezermzgl@gmail.com'  \n",
        "    }\n",
        "    \n",
        "    response = requests.get(url, params=params, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        json_data = response.json()\n",
        "        if json_data:\n",
        "            address = json_data[0].get('address', {})\n",
        "            region = address.get('state')  \n",
        "            return region\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Region'] = None\n",
        "\n",
        "for index, row in df.head(5).iterrows():\n",
        "    adres = f\"{row['Address']}, {row['Postal Code']}, {row['City']}, {row['Country']}\"\n",
        "    region = get_region(adres)\n",
        "    \n",
        "    df.at[index, 'Region'] = region\n",
        "    print(f\"Processed {row['restaurantName']}: Region = {region}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_m1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
